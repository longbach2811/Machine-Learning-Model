import numpy as np
import matplotlib.pyplot as plt


class Layer():
    def __init__(self, n_cells, n_inputs):
        self.w = np.random.random((n_inputs, n_cells)) 

class NeuralNetwork():
    def __init__(self, layer1, layer2):
        self.layer1 = layer1
        self.layer2 = layer2

    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))

    def sigmoid_derivative(self, z):
        return z * (1 - z)  

    def predict(self, inputs):
        print("input shape: ",inputs.shape)
        ouput_layer_1 = self.sigmoid(np.dot(inputs, self.layer1.w))
        print("output layer 1 shape: ", ouput_layer_1.shape)
        ouput_layer_2 = self.sigmoid(np.dot(ouput_layer_1, self.layer2.w))
        print("output layer 2 shape: ", ouput_layer_2.shape)
        return ouput_layer_1, ouput_layer_2

    def train(self, training_inputs, training_outputs, lr, n_iters):
        for _ in range(n_iters):
            output_layer_1, output_layer_2 = self.predict(training_inputs)

            layer2_lost = training_outputs - output_layer_2
            layer2_delta = layer2_lost * self.sigmoid_derivative(output_layer_2)

            layer1_lost = np.dot(layer2_delta, self.layer2.w.T) 
            layer1_delta = layer1_lost * self.sigmoid_derivative(output_layer_1)

            layer1_dw = np.dot(training_inputs.T, layer1_delta)
            layer2_dw = np.dot(output_layer_1.T, layer2_delta)

            self.layer1.w -= lr * layer1_dw
            self.layer2.w -= lr * layer2_dw

if __name__ == "__main__":
    training_inputs = np.array([ [0.0006270560902939734,1.5524641502825586e-09,2.2628236948620886e-14,2.1178728951225787e-17,-1.7587639598804865e-33,-5.850736902990582e-23,0.0],
                                 [0.0006390769051134845,-5.854628726618293e-08,4.329503783643097e-13,3.711610337859238e-15,-1.5859313901623613e-28,1.695070850025747e-19,0.0], 
                                 [0.0006287564968818239,-2.290260301418379e-08,1.8907612558882032e-13,6.921072034872136e-16,3.343937906307813e-30,2.6628558451303558e-20,0.0], 
                                 [0.0006311873409749329,1.9401339905001645e-09,1.651388608741984e-12,7.984283450545093e-16,-3.2233260528238875e-29,-2.091782978058894e-21,0.0], 
                                 [0.0006357614414246474,2.216398335411433e-08,1.7174030480213738e-12,1.1117804694154713e-14,-4.3774219673729595e-28,-5.774401662534116e-19,0.0],
                                 [0.0006345829916796213,-4.2803396638995174e-08,1.2438073163878933e-12,2.3055101692266653e-15,9.609901104993568e-29,-1.5362344247977668e-19,0.0],
                                 [0.0006456002861099135,8.441197719880073e-08,3.722743772638658e-12,4.932742501622579e-14,2.242184874517105e-26,5.798859735307701e-18,0.0],
                                 [0.0006431238059135554,-2.406463067604168e-08,8.213329745195082e-12,9.430385594053848e-14,9.07163574995218e-26,6.162575794260851e-18,0.0],
                                 [0.0012129232978590084,-5.674790130843133e-07,3.6934792611819044e-10,6.707046273379974e-10,6.510018004093274e-19,4.3814427348460576e-13,0.0],
                                 [0.0006638740564313683,1.2925646562893087e-08,6.908795666132662e-13,4.823838746907026e-12,1.9025771506918487e-23,1.0593708513584721e-16,0.0], 
                                 [0.0006732341359319697,1.4771305923723319e-09,2.158647261503307e-12,8.220371820421277e-14,2.8488768412981465e-26,-2.9562568950671588e-18,0.0],
                                 [0.0015469072175038007,-2.2647072596600193e-08,9.180639375581012e-13,3.649629165243467e-12,8.848888023165385e-24,-8.483956722555567e-17,0.0],
                                 [0.0019431450848995562,1.488825198724153e-07,2.159353907566524e-11,4.2154752817499027e-11,-6.38435878302127e-22,-1.9797108631926962e-15,0.0],
                                 [0.0006676839764572478,-2.605134229815932e-09,1.070692557804248e-13,4.6835871477102955e-15,-1.1428486248624838e-28,-1.559444920454599e-20,0.0],
                                 [0.0012211978587090598,3.389056312925093e-07,1.8027718355827773e-11,8.281720694729726e-10,5.494284682572702e-19,2.2981786482694737e-13,0.0],
                                 [0.0009726076375232199,2.851534208650472e-07,6.489626762222176e-11,7.21704513662392e-10,3.259111816056636e-19,2.0269339791307152e-13,0.0]])
    training_outputs = np.array([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]).T
    np.random.seed(1)
    layer1 = Layer(15, 7)
    layer2 = Layer(1, 15)

    nn = NeuralNetwork(layer1, layer2)

    nn.train(training_inputs, training_outputs, 1, 6000)
    hidden_state, output = nn.predict(np.array([0.0006368761628879587,-1.3963843060435475e-08,7.835630612207705e-13,1.0163240135890744e-14,1.0382275122847265e-27,3.71161405511919e-20,0.0]))
    print(hidden_state, output)
